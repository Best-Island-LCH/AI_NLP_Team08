"""
Curriculum Learning for AI Quality Evaluation

ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì„ ì ìš©í•œ AI í’ˆì§ˆ í‰ê°€
- ë‚œì´ë„ ê¸°ë°˜ ìƒ˜í”Œ ì •ë ¬
- ì—í­ë³„ ì ì§„ì  í•™ìŠµ
"""

import torch
import torch.nn as nn
import numpy as np
from torch.utils.data import Dataset, Sampler, DataLoader
from typing import List, Dict, Optional
from sklearn.metrics import f1_score


class CurriculumDataset(Dataset):
    """
    ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹
    ê° ìƒ˜í”Œì˜ ë‚œì´ë„ ì •ë³´ë¥¼ í¬í•¨
    """
    
    CRITERIA = [
        'linguistic_acceptability', 'consistency', 'interestingness',
        'unbias', 'harmlessness', 'no_hallucination',
        'understandability', 'sensibleness', 'specificity'
    ]
    
    def __init__(self, samples: List[dict], tokenizer, max_length: int = 512):
        """
        Args:
            samples: ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ (dict í˜•íƒœ)
            tokenizer: HuggingFace í† í¬ë‚˜ì´ì €
            max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        """
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # ê° ìƒ˜í”Œì˜ ë‚œì´ë„ ê³„ì‚°
        self.difficulties = self._compute_difficulties()
    
    def _compute_difficulties(self) -> np.ndarray:
        """í‰ê°€ì ì¼ì¹˜ë„ ê¸°ë°˜ ë‚œì´ë„ ê³„ì‚°"""
        difficulties = []
        
        for sample in self.samples:
            criterion_difficulties = []
            
            for c in self.CRITERIA:
                unanimous_key = f'{c}_unanimous'
                if unanimous_key in sample:
                    # ë§Œì¥ì¼ì¹˜ê°€ ì•„ë‹ˆë©´ ì–´ë ¤ì›€
                    criterion_difficulties.append(
                        0 if sample[unanimous_key] == 1 else 1
                    )
                else:
                    # unanimous ì •ë³´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
                    criterion_difficulties.append(0.5)
            
            # í‰ê·  ë‚œì´ë„
            difficulty = np.mean(criterion_difficulties)
            difficulties.append(difficulty)
        
        return np.array(difficulties)
    
    def get_easy_indices(self, threshold: float = 0.3) -> np.ndarray:
        """ì‰¬ìš´ ìƒ˜í”Œ ì¸ë±ìŠ¤ (ë‚œì´ë„ <= threshold)"""
        return np.where(self.difficulties <= threshold)[0]
    
    def get_medium_indices(self, low: float = 0.3, high: float = 0.7) -> np.ndarray:
        """ì¤‘ê°„ ë‚œì´ë„ ìƒ˜í”Œ ì¸ë±ìŠ¤"""
        return np.where(
            (self.difficulties > low) & (self.difficulties <= high)
        )[0]
    
    def get_hard_indices(self, threshold: float = 0.7) -> np.ndarray:
        """ì–´ë ¤ìš´ ìƒ˜í”Œ ì¸ë±ìŠ¤ (ë‚œì´ë„ > threshold)"""
        return np.where(self.difficulties > threshold)[0]
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx: int):
        sample = self.samples[idx]
        
        encoding = self.tokenizer(
            sample['input_text'],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # _majority ì ‘ë¯¸ì‚¬ê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        labels = []
        for c in self.CRITERIA:
            if f'{c}_majority' in sample:
                labels.append(sample[f'{c}_majority'])
            elif c in sample:
                labels.append(sample[c])
            else:
                labels.append(0)
        
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(labels, dtype=torch.float),
            'difficulty': self.difficulties[idx]
        }


class CurriculumSampler(Sampler):
    """
    ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì„ ìœ„í•œ ìƒ˜í”ŒëŸ¬
    ì—í­ì— ë”°ë¼ í¬í•¨í•  ìƒ˜í”Œ ê²°ì •
    """
    
    def __init__(
        self, 
        dataset: CurriculumDataset, 
        total_epochs: int,
        strategy: str = 'linear'
    ):
        """
        Args:
            dataset: CurriculumDataset ì¸ìŠ¤í„´ìŠ¤
            total_epochs: ì´ í•™ìŠµ ì—í­ ìˆ˜
            strategy: ì»¤ë¦¬í˜ëŸ¼ ì „ëµ ('linear', 'sqrt', 'step')
        """
        self.dataset = dataset
        self.total_epochs = total_epochs
        self.strategy = strategy
        self.current_epoch = 0
        
        # ë‚œì´ë„ë³„ ì¸ë±ìŠ¤
        self.easy_indices = dataset.get_easy_indices()
        self.medium_indices = dataset.get_medium_indices()
        self.hard_indices = dataset.get_hard_indices()
        
        print(f"ğŸ“Š ë‚œì´ë„ ë¶„í¬:")
        print(f"   Easy: {len(self.easy_indices)} ({len(self.easy_indices)/len(dataset)*100:.1f}%)")
        print(f"   Medium: {len(self.medium_indices)} ({len(self.medium_indices)/len(dataset)*100:.1f}%)")
        print(f"   Hard: {len(self.hard_indices)} ({len(self.hard_indices)/len(dataset)*100:.1f}%)")
    
    def set_epoch(self, epoch: int):
        """í˜„ì¬ ì—í­ ì„¤ì • (í•™ìŠµ ë£¨í”„ì—ì„œ í˜¸ì¶œ)"""
        self.current_epoch = epoch
    
    def _get_competence(self) -> float:
        """
        í˜„ì¬ ì—í­ì˜ ì—­ëŸ‰(competence) ê³„ì‚°
        ì—­ëŸ‰ì— ë”°ë¼ í¬í•¨í•  ë‚œì´ë„ ê²°ì •
        """
        if self.strategy == 'linear':
            # ì„ í˜• ì¦ê°€
            return min(1.0, (self.current_epoch + 1) / self.total_epochs)
        elif self.strategy == 'sqrt':
            # ì œê³±ê·¼ (ì´ˆê¸°ì— ë¹ ë¥´ê²Œ, í›„ê¸°ì— ëŠë¦¬ê²Œ)
            return min(1.0, np.sqrt((self.current_epoch + 1) / self.total_epochs))
        elif self.strategy == 'step':
            # ë‹¨ê³„ì 
            if self.current_epoch < self.total_epochs // 3:
                return 0.33
            elif self.current_epoch < 2 * self.total_epochs // 3:
                return 0.66
            else:
                return 1.0
        else:
            return 1.0
    
    def __iter__(self):
        competence = self._get_competence()
        
        # ì—­ëŸ‰ì— ë”°ë¼ í¬í•¨í•  ìƒ˜í”Œ ê²°ì •
        if competence < 0.33:
            # ì‰¬ìš´ ìƒ˜í”Œë§Œ
            indices = self.easy_indices.copy()
        elif competence < 0.66:
            # ì‰¬ìš´ + ì¤‘ê°„
            indices = np.concatenate([self.easy_indices, self.medium_indices])
        else:
            # ëª¨ë“  ìƒ˜í”Œ
            indices = np.arange(len(self.dataset))
        
        # ì…”í”Œ
        np.random.shuffle(indices)
        
        print(f"   Epoch {self.current_epoch}: competence={competence:.2f}, "
              f"samples={len(indices)}/{len(self.dataset)}")
        
        return iter(indices.tolist())
    
    def __len__(self):
        competence = self._get_competence()
        
        if competence < 0.33:
            return len(self.easy_indices)
        elif competence < 0.66:
            return len(self.easy_indices) + len(self.medium_indices)
        else:
            return len(self.dataset)


class CurriculumTrainer:
    """
    ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ íŠ¸ë ˆì´ë„ˆ
    """
    
    def __init__(
        self, 
        model: nn.Module, 
        train_dataset: CurriculumDataset, 
        val_loader: DataLoader, 
        device: torch.device, 
        config: dict
    ):
        """
        Args:
            model: PyTorch ëª¨ë¸
            train_dataset: CurriculumDataset ì¸ìŠ¤í„´ìŠ¤
            val_loader: ê²€ì¦ ë°ì´í„°ë¡œë”
            device: í•™ìŠµ ë””ë°”ì´ìŠ¤
            config: í•™ìŠµ ì„¤ì •
                - epochs: ì´ ì—í­ ìˆ˜
                - batch_size: ë°°ì¹˜ í¬ê¸°
                - learning_rate: í•™ìŠµë¥ 
                - curriculum_strategy: ì»¤ë¦¬í˜ëŸ¼ ì „ëµ
        """
        self.model = model.to(device)
        self.train_dataset = train_dataset
        self.val_loader = val_loader
        self.device = device
        self.config = config
        
        # ì»¤ë¦¬í˜ëŸ¼ ìƒ˜í”ŒëŸ¬
        self.sampler = CurriculumSampler(
            train_dataset,
            total_epochs=config['epochs'],
            strategy=config.get('curriculum_strategy', 'linear')
        )
        
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config['learning_rate']
        )
        
        self.criterion = nn.BCEWithLogitsLoss()
    
    def train(self):
        """ì „ì²´ í•™ìŠµ"""
        best_f1 = 0
        
        print(f"\nğŸ“ Curriculum Learning ì‹œì‘")
        print(f"   ì „ëµ: {self.config.get('curriculum_strategy', 'linear')}")
        print(f"   ì—í­: {self.config['epochs']}")
        
        for epoch in range(self.config['epochs']):
            # ìƒ˜í”ŒëŸ¬ì— í˜„ì¬ ì—í­ ì•Œë¦¼
            self.sampler.set_epoch(epoch)
            
            # ë°ì´í„°ë¡œë” ìƒì„± (ë§¤ ì—í­ë§ˆë‹¤ ìƒˆë¡œ)
            train_loader = DataLoader(
                self.train_dataset,
                batch_size=self.config['batch_size'],
                sampler=self.sampler
            )
            
            # í•™ìŠµ
            train_loss = self._train_epoch(train_loader)
            
            # í‰ê°€
            val_results = self._evaluate()
            
            print(f"   Train Loss: {train_loss:.4f} | Val F1: {val_results['f1_macro']:.4f}")
            
            if val_results['f1_macro'] > best_f1:
                best_f1 = val_results['f1_macro']
                torch.save(self.model.state_dict(), 'best_curriculum_model.pt')
                print("   âœ… Best model saved!")
        
        print(f"\nğŸ† Best F1: {best_f1:.4f}")
        return best_f1
    
    def _train_epoch(self, loader: DataLoader) -> float:
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        total_loss = 0
        
        for batch in loader:
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            self.optimizer.zero_grad()
            logits = self.model(input_ids, attention_mask)
            loss = self.criterion(logits, labels)
            loss.backward()
            
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / len(loader)
    
    def _evaluate(self) -> dict:
        """ê²€ì¦ ë°ì´í„° í‰ê°€"""
        self.model.eval()
        all_preds, all_labels = [], []
        
        with torch.no_grad():
            for batch in self.val_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels']
                
                logits = self.model(input_ids, attention_mask)
                preds = (torch.sigmoid(logits) > 0.5).float()
                
                all_preds.append(preds.cpu())
                all_labels.append(labels)
        
        preds = torch.cat(all_preds).numpy()
        labels = torch.cat(all_labels).numpy()
        
        return {
            'f1_macro': f1_score(labels, preds, average='macro', zero_division=0)
        }


# ============================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================================

def example_usage():
    """Curriculum Learning ì‚¬ìš© ì˜ˆì‹œ"""
    
    print("Curriculum Learning ì˜ˆì‹œ")
    print("=" * 50)
    
    # ë”ë¯¸ ìƒ˜í”Œ ìƒì„±
    samples = []
    for i in range(100):
        # Easy ìƒ˜í”Œ (ë§Œì¥ì¼ì¹˜)
        if i < 40:
            sample = {
                'input_text': f'ì‰¬ìš´ ìƒ˜í”Œ {i}',
                'linguistic_acceptability_majority': 1,
                'linguistic_acceptability_unanimous': 1,
                'consistency_majority': 1,
                'consistency_unanimous': 1,
                # ... ë‚˜ë¨¸ì§€ ê¸°ì¤€ë“¤ë„ unanimous=1
            }
            for c in CurriculumDataset.CRITERIA:
                sample[f'{c}_majority'] = 1
                sample[f'{c}_unanimous'] = 1
        # Hard ìƒ˜í”Œ (ë¶ˆì¼ì¹˜)
        else:
            sample = {
                'input_text': f'ì–´ë ¤ìš´ ìƒ˜í”Œ {i}',
            }
            for c in CurriculumDataset.CRITERIA:
                sample[f'{c}_majority'] = np.random.randint(0, 2)
                sample[f'{c}_unanimous'] = 0  # ë¶ˆì¼ì¹˜
        
        samples.append(sample)
    
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = CurriculumDataset(samples, tokenizer)
    
    # ìƒ˜í”ŒëŸ¬ í…ŒìŠ¤íŠ¸
    sampler = CurriculumSampler(
        dataset,
        total_epochs=10,
        strategy='sqrt'
    )
    
    print("\nì—í­ë³„ ìƒ˜í”Œ ìˆ˜:")
    for epoch in range(10):
        sampler.set_epoch(epoch)
        indices = list(sampler)
        # (ì¶œë ¥ì€ __iter__ì—ì„œ ìë™ìœ¼ë¡œ)


if __name__ == "__main__":
    example_usage()
