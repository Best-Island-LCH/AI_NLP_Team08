# -*- coding: utf-8 -*-
"""base_mtl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FoShU5MiKzWva5LEWmQy1IPRW30yq1o5

# AI 품질 평가 모델 학습 - klue/roberta-base

이 노트북은 AI 응답 품질 평가를 위한 Multi-label Classification 모델을 학습합니다.

**평가 기준 (9개)**:

- linguistic_acceptability (언어적 수용성)
- consistency (일관성)
- interestingness (흥미로움)
- unbias (편향 없음)
- harmlessness (무해성)
- no_hallucination (환각 없음)
- understandability (이해 가능성)
- sensibleness (합리성)
- specificity (구체성)

## 1. 라이브러리 설치 및 임포트
"""

!pip install torch transformers datasets pandas scikit-learn tqdm -q

from google.colab import drive
drive.mount('/content/drive/')

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, #텍스트->토큰ID 변환기, BERT모델 쓰기
    AutoModelForSequenceClassification, #프로젝트 모델 인코더/CLS/리니어(MLP)/로짓
    TrainingArguments,#학습 설정 객체
    Trainer,#학습 실행
)
from sklearn.metrics import accuracy_score, f1_score, classification_report
#맞춘 비율, 클래스 불균형, 전부 출력
from tqdm.auto import tqdm #전처리, 진행률 표시
import warnings

warnings.filterwarnings('ignore') #실험 로그 가독성

# GPU 확인
device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
print(f"Using device: {device}") #CSV 기반 텍스트 데이터를 KLUE-RoBERTa 분류 모델로 학습하고, HuggingFace Trainer로 표준 실험을 돌리는 베이스라인 파이프라인의 준비 단계

"""## 2. 설정"""

# 모델 및 학습 설정
MODEL_NAME = "klue/roberta-base"
MAX_LENGTH = 256
BATCH_SIZE = 256  # A100 40GB/80GB라면 충분히 가능합니다.
LEARNING_RATE = 1e-4  # 배치가 256으로 매우 크므로, 3e-5보다는 1e-4 정도로 높여야 학습이 잘 됩니다.
NUM_EPOCHS = 15   # Early Stopping을 쓸 것이므로 넉넉하게 15~20으로 잡으세요.
SEED = 42

# 평가 기준 (타겟 컬럼)
CRITERIA = [
    'linguistic_acceptability',
    'consistency',
    'interestingness',
    'unbias',
    'harmlessness',
    'no_hallucination',
    'understandability',
    'sensibleness',
    'specificity'
]
NUM_LABELS = len(CRITERIA)

# 시드 고정
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED) # GPU 가속 시 모든 코어 시드 고정
np.random.seed(SEED)

# A100 전용 가속 설정 (성능 향상을 위해 추가하세요)
torch.backends.cuda.matmul.allow_tf32 = True # A100의 TF32 모드 활성화
torch.backends.cudnn.allow_tf32 = True

"""## 3. 데이터 로드"""

# # unzip
# unzip_path = '/content/drive/MyDrive/NLP_03_PJ1/data.zip'
# !unzip -qq {unzip_path} -d /content/drive/MyDrive/NLP_03_PJ1/

# Training 및 Validation 데이터 로드 (aggregated 버전 사용 - majority voting 결과)
train_df = pd.read_csv('/content/drive/MyDrive/NLP_03_PJ1/data/train/training_all_aggregated.csv', encoding='utf-8-sig')
val_df = pd.read_csv('/content/drive/MyDrive/NLP_03_PJ1/data/val/validation_all_aggregated.csv', encoding='utf-8-sig')

print(f"Training 데이터: {len(train_df):,}개")
print(f"Validation 데이터: {len(val_df):,}개")

# 데이터 확인
train_df.head()

# 컬럼 확인
print("컬럼 목록:")
print(train_df.columns.tolist())

"""## 4. 데이터 전처리"""

def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:
    """데이터 전처리: 입력 텍스트 생성 및 결측치 처리"""
    df = df.copy()

    # 결측치 처리
    df['human_question'] = df['human_question'].fillna('')
    df['bot_response'] = df['bot_response'].fillna('')

    # 입력 텍스트 생성: [질문] + [SEP] + [응답]
    df['input_text'] = df['human_question'] + ' [SEP] ' + df['bot_response']

    # 타겟 컬럼 추출 (majority voting 결과)
    target_cols = [f'{c}_majority' for c in CRITERIA]

    # 타겟 결측치 제거
    df = df.dropna(subset=target_cols)

    return df

train_df = preprocess_data(train_df)
val_df = preprocess_data(val_df)

print(f"전처리 후 Training 데이터: {len(train_df):,}개")
print(f"전처리 후 Validation 데이터: {len(val_df):,}개")

# 레이블 분포 확인
target_cols = [f'{c}_majority' for c in CRITERIA]

print("=" * 50)
print("레이블 분포 (Training)")
print("=" * 50)
for col in target_cols:
    pos_ratio = train_df[col].mean()
    print(f"{col}: {pos_ratio:.2%} positive")

"""## 5. Tokenizer 로드"""

# klue/roberta-base 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

print(f"Tokenizer 로드 완료: {MODEL_NAME}")
print(f"Vocab size: {tokenizer.vocab_size:,}")

# 토크나이저 테스트
sample_text = train_df['input_text'].iloc[0]
print(f"샘플 텍스트:\n{sample_text}\n")

tokens = tokenizer(sample_text, truncation=True, max_length=MAX_LENGTH)
print(f"토큰 수: {len(tokens['input_ids'])}")

"""## 6. Dataset 클래스 정의"""

class QualityEvalDataset(Dataset):
    """AI 품질 평가 데이터셋"""

    def __init__(self, df: pd.DataFrame, tokenizer, max_length: int = 256):
        self.df = df.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.target_cols = [f'{c}_majority' for c in CRITERIA]

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        text = row['input_text']
        labels = row[self.target_cols].values.astype(np.float32)

        # 토크나이즈
        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.tensor(labels, dtype=torch.float)
        }
# ==========================================
# 6.5 Multi-task 모델 및 손실 함수 정의
# ==========================================

import torch.nn as nn
class CriterionInteractionLayer(nn.Module):
    def __init__(self, num_criteria=9, hidden_dim=64):
        super().__init__()
        self.criterion_embeddings = nn.Parameter(torch.randn(num_criteria, hidden_dim))
        self.adjustment = nn.Sequential(
            nn.Linear(num_criteria + hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_criteria)
        )

    def forward(self, logits):
        batch_size = logits.size(0)
        criterion_emb = self.criterion_embeddings.unsqueeze(0).expand(batch_size, -1, -1)
        combined_mean = criterion_emb.mean(dim=1)
        adjustment_input = torch.cat([logits, combined_mean], dim=-1)
        adjustment = self.adjustment(adjustment_input)
        return logits + 0.1 * adjustment

class MultiTaskQualityModel(nn.Module):
    def __init__(self, model_name='klue/roberta-base', num_criteria=9, use_interaction=True):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        hidden_size = self.encoder.config.hidden_size
        self.use_interaction = use_interaction

        # 각 지표별 독립적인 분류 헤드
        self.task_heads = nn.ModuleList([
            nn.Sequential(
                nn.Dropout(0.1),
                nn.Linear(hidden_size, 128),
                nn.GELU(),
                nn.Linear(128, 1)
            ) for _ in range(num_criteria)
        ])

        if use_interaction:
            self.interaction_layer = CriterionInteractionLayer(num_criteria)

    def forward(self, input_ids, attention_mask, labels=None, **kwargs):
        # Shared encoding
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # CLS 토큰 추출
        cls_output = outputs.last_hidden_state[:, 0, :]

        # 각 헤드별 예측
        task_outputs = [head(cls_output) for head in self.task_heads]
        logits = torch.cat(task_outputs, dim=1)

        # Interaction Layer 적용
        if self.use_interaction:
            logits = self.interaction_layer(logits)

        return logits

# Dataset 생성
train_dataset = QualityEvalDataset(train_df, tokenizer, MAX_LENGTH)
val_dataset = QualityEvalDataset(val_df, tokenizer, MAX_LENGTH)

print(f"Train dataset size: {len(train_dataset):,}")
print(f"Val dataset size: {len(val_dataset):,}")

# Dataset 확인
sample = train_dataset[0]
print(f"input_ids shape: {sample['input_ids'].shape}")
print(f"attention_mask shape: {sample['attention_mask'].shape}")
print(f"labels shape: {sample['labels'].shape}")
print(f"labels: {sample['labels']}")

# 토큰 번호를 다시 단어로 바꿔서 확인해보기
print(f"원문 복구: {tokenizer.decode(sample['input_ids'])}")

# 패딩이 어떻게 채워졌는지 직접 확인
print(f"실제 데이터: {sample['input_ids']}")

"""## 7. 모델 로드"""

# 기존 임포트 부분에 AutoModel을 추가합니다.
from transformers import (
    AutoTokenizer,
    AutoModel,  # <--- 이 부분이 반드시 추가되어야 합니다!
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)
import torch.nn as nn # 앞서 발생한 에러 방지용
# 7. 모델 로드 (수정됨)
model = MultiTaskQualityModel(MODEL_NAME, num_criteria=NUM_LABELS)
model.to(device)

# 불확실성 기반 가중치를 학습하기 위한 손실 함수 설정
class MTLLossWrapper(nn.Module):
    def __init__(self, num_tasks=9):
        super().__init__()
        self.log_vars = nn.Parameter(torch.zeros(num_tasks))
        self.bce = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, logits, targets):
        task_losses = self.bce(logits, targets).mean(dim=0)
        precision = torch.exp(-self.log_vars)
        loss = (precision * task_losses + self.log_vars).sum()
        return loss

loss_fn = MTLLossWrapper(num_tasks=NUM_LABELS).to(device)

"""## 8. 평가 지표 함수"""

def compute_metrics(eval_pred):
    """평가 지표 계산"""
    predictions, labels = eval_pred

    # Sigmoid 적용 후 0.5 기준으로 이진화
    predictions = torch.sigmoid(torch.tensor(predictions)).numpy()
    predictions = (predictions > 0.5).astype(int)
    labels = labels.astype(int)

    # 전체 정확도 (모든 레이블이 일치해야 정답)
    exact_match = np.all(predictions == labels, axis=1).mean()

    # 레이블별 정확도
    per_label_acc = (predictions == labels).mean(axis=0)

    # Micro/Macro F1
    micro_f1 = f1_score(labels, predictions, average='micro')
    macro_f1 = f1_score(labels, predictions, average='macro')

    metrics = {
        'exact_match': exact_match,
        'micro_f1': micro_f1,
        'macro_f1': macro_f1,
    }

    # 각 기준별 정확도 추가
    for i, criterion in enumerate(CRITERIA):
        metrics[f'{criterion}_acc'] = per_label_acc[i]

    return metrics

"""## 9. Trainer 설정 및 학습"""

from transformers import EarlyStoppingCallback

from transformers import EarlyStoppingCallback, Trainer

# 1. Training Arguments 수정
training_args = TrainingArguments(
    output_dir='./outputs/klue-roberta-base-mtl',
    num_train_epochs=20,              # 에포크를 넉넉하게 늘립니다 (조기 종료가 알아서 멈춤)
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE * 2,
    learning_rate=LEARNING_RATE,
    weight_decay=0.01,
    warmup_ratio=0.1,
    logging_dir='./logs',
    logging_steps=50,                 # 로그를 조금 더 자주 확인
    eval_strategy='epoch',            # 에포크마다 평가 (필수)
    save_strategy='epoch',            # 에포크마다 저장 (필수)
    load_best_model_at_end=True,      # 가장 성능 좋은 모델을 마지막에 로드 (필수)
    metric_for_best_model='macro_f1', # 조기 종료의 기준 지표
    greater_is_better=True,
    report_to='none',
    seed=SEED,
    bf16=True,  # A100은 fp16보다 bf16에서 훨씬 빠르고 안정적입니다.
    tf32=True,  # TF32 가속 사용
)

# 2. MTLTrainer 클래스는 그대로 사용 (임포트만 확인)
class MTLTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.pop("labels")
        logits = model(**inputs)
        loss = loss_fn(logits, labels)
        return (loss, {"logits": logits}) if return_outputs else loss

# 3. Trainer 생성 시 callbacks 추가
trainer = MTLTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    # 여기에 EarlyStoppingCallback을 추가합니다.
    # early_stopping_patience=3 : 3번의 에포크 동안 성적이 안 오르면 중단!
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# 학습 시작
print("학습 시작...")
train_result = trainer.train()
print("학습 완료!")

"""## 10. 평가"""

# Validation 데이터로 평가
eval_results = trainer.evaluate()

print("=" * 50)
print("평가 결과")
print("=" * 50)
for key, value in eval_results.items():
    if 'loss' in key or 'f1' in key or 'match' in key:
        print(f"{key}: {value:.4f}")

# 기준별 정확도 출력
print("\n" + "=" * 50)
print("기준별 정확도")
print("=" * 50)
for criterion in CRITERIA:
    key = f'eval_{criterion}_acc'
    if key in eval_results:
        print(f"{criterion}: {eval_results[key]:.4f}")

"""## 11. 모델 저장"""

# 모델 저장
save_path = './outputs/klue-roberta-base-final'
trainer.save_model(save_path)
tokenizer.save_pretrained(save_path)

print(f"모델 저장 완료: {save_path}")

"""## 12. 추론 예시"""

def predict(text: str, model, tokenizer, device):
    """단일 텍스트에 대한 예측 수행"""
    model.eval()

    encoding = tokenizer(
        text,
        truncation=True,
        max_length=MAX_LENGTH,
        padding='max_length',
        return_tensors='pt'
    )

    encoding = {k: v.to(device) for k, v in encoding.items()}

    with torch.no_grad():
        logits = model(**encoding) # outputs 자체가 이미 logits(텐서)입니다.
        probs = torch.sigmoid(logits).cpu().numpy()[0]
        preds = (probs > 0.5).astype(int)

    results = {}
    for i, criterion in enumerate(CRITERIA):
        results[criterion] = {
            'prediction': int(preds[i]),
            'probability': float(probs[i])
        }

    return results

# 추론 테스트
sample_question = "한국의 수도는 어디야?"
sample_response = "한국의 수도는 서울입니다. 서울은 대한민국의 정치, 경제, 문화의 중심지로, 약 1000만 명의 인구가 거주하고 있습니다."
sample_input = f"{sample_question} [SEP] {sample_response}" #다른 모델을 사용할 경우 전처리 함수를 가져올 수 있음

print(f"질문: {sample_question}")
print(f"응답: {sample_response}")
print("\n" + "=" * 50)
print("예측 결과")
print("=" * 50)

results = predict(sample_input, model, tokenizer, device)
for criterion, values in results.items():
    status = "✓" if values['prediction'] == 1 else "✗"
    print(f"{status} {criterion}: {values['probability']:.2%}")