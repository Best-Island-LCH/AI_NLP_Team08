"""
í† í° ë¶„í¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (Phase 0)

ê° ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¡œ ë°ì´í„°ì…‹ì˜ í† í° ê¸¸ì´ ë¶„í¬ë¥¼ ë¶„ì„í•˜ì—¬
ì ì • max_lengthë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
from transformers import AutoTokenizer
from tqdm import tqdm
import json
from pathlib import Path
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings('ignore')

# ë¶„ì„í•  ëª¨ë¸ë“¤
MODELS = {
    'klue/roberta-base': 'klue/roberta-base',
    'monologg/koelectra-base-v3-discriminator': 'monologg/koelectra-base-v3-discriminator',
    'monologg/kobert': 'monologg/kobert',
    'klue/roberta-large': 'klue/roberta-large',
}

# í‰ê°€ ê¸°ì¤€
CRITERIA = [
    'linguistic_acceptability', 'consistency', 'interestingness',
    'unbias', 'harmlessness', 'no_hallucination',
    'understandability', 'sensibleness', 'specificity'
]


def load_data(data_dir: str):
    """ë°ì´í„° ë¡œë“œ"""
    train_path = Path(data_dir) / 'train' / 'training_all_aggregated.csv'
    val_path = Path(data_dir) / 'val' / 'validation_all_aggregated.csv'
    
    train_df = pd.read_csv(train_path, encoding='utf-8-sig')
    val_df = pd.read_csv(val_path, encoding='utf-8-sig')
    
    print(f"Train samples: {len(train_df):,}")
    print(f"Val samples: {len(val_df):,}")
    
    return train_df, val_df


def preprocess_text(df: pd.DataFrame) -> pd.DataFrame:
    """ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„±"""
    df = df.copy()
    df['human_question'] = df['human_question'].fillna('')
    df['bot_response'] = df['bot_response'].fillna('')
    df['input_text'] = df['human_question'] + ' [SEP] ' + df['bot_response']
    return df


def analyze_token_distribution(df: pd.DataFrame, tokenizer, text_column: str = 'input_text', 
                                sample_size: int = None):
    """í† í° ê¸¸ì´ ë¶„í¬ ë¶„ì„"""
    if sample_size and len(df) > sample_size:
        df_sample = df.sample(n=sample_size, random_state=42)
    else:
        df_sample = df
    
    lengths = []
    for text in tqdm(df_sample[text_column], desc="Tokenizing"):
        if pd.isna(text) or text == '':
            lengths.append(0)
            continue
        try:
            tokens = tokenizer(str(text), truncation=False, add_special_tokens=True)
            lengths.append(len(tokens['input_ids']))
        except Exception as e:
            print(f"Error tokenizing: {e}")
            lengths.append(0)
    
    lengths = np.array(lengths)
    
    stats = {
        'count': len(lengths),
        'mean': float(np.mean(lengths)),
        'std': float(np.std(lengths)),
        'min': int(np.min(lengths)),
        'p25': int(np.percentile(lengths, 25)),
        'p50': int(np.percentile(lengths, 50)),
        'p75': int(np.percentile(lengths, 75)),
        'p90': int(np.percentile(lengths, 90)),
        'p95': int(np.percentile(lengths, 95)),
        'p99': int(np.percentile(lengths, 99)),
        'max': int(np.max(lengths)),
    }
    
    # ê¶Œì¥ max_length ê³„ì‚°
    stats['recommended_max_lengths'] = {
        'conservative': stats['p90'],  # 90% ì»¤ë²„ë¦¬ì§€
        'balanced': stats['p95'],      # 95% ì»¤ë²„ë¦¬ì§€
        'aggressive': stats['p99'],    # 99% ì»¤ë²„ë¦¬ì§€
    }
    
    return stats, lengths


def plot_distribution(lengths_dict: dict, output_path: str):
    """í† í° ê¸¸ì´ ë¶„í¬ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()
    
    for idx, (model_name, lengths) in enumerate(lengths_dict.items()):
        ax = axes[idx]
        ax.hist(lengths, bins=50, edgecolor='black', alpha=0.7)
        ax.axvline(np.percentile(lengths, 90), color='r', linestyle='--', label=f'P90: {int(np.percentile(lengths, 90))}')
        ax.axvline(np.percentile(lengths, 95), color='g', linestyle='--', label=f'P95: {int(np.percentile(lengths, 95))}')
        ax.axvline(np.percentile(lengths, 99), color='b', linestyle='--', label=f'P99: {int(np.percentile(lengths, 99))}')
        ax.set_title(f'{model_name}', fontsize=12)
        ax.set_xlabel('Token Length')
        ax.set_ylabel('Frequency')
        ax.legend()
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"Distribution plot saved to: {output_path}")


def main():
    # ë°ì´í„° ê²½ë¡œ
    data_dir = Path(__file__).parent.parent.parent / 'data'
    output_dir = Path(__file__).parent.parent / 'config'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("í† í° ë¶„í¬ ë¶„ì„ (Phase 0)")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë“œ
    train_df, val_df = load_data(data_dir)
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    train_df = preprocess_text(train_df)
    val_df = preprocess_text(val_df)
    
    # ì „ì²´ ë°ì´í„° í•©ì¹˜ê¸°
    all_df = pd.concat([train_df, val_df], ignore_index=True)
    print(f"\nTotal samples: {len(all_df):,}")
    
    # ìƒ˜í”Œ í¬ê¸° (ì „ì²´ ë°ì´í„°ê°€ í¬ë©´ ìƒ˜í”Œë§)
    sample_size = min(50000, len(all_df))
    print(f"Analyzing {sample_size:,} samples...")
    
    results = {}
    lengths_dict = {}
    
    for model_key, model_name in MODELS.items():
        print(f"\n{'=' * 60}")
        print(f"Analyzing: {model_name}")
        print("=" * 60)
        
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            stats, lengths = analyze_token_distribution(all_df, tokenizer, sample_size=sample_size)
            results[model_key] = stats
            lengths_dict[model_key] = lengths
            
            print(f"\nğŸ“Š Statistics for {model_name}:")
            print(f"  Mean: {stats['mean']:.1f} Â± {stats['std']:.1f}")
            print(f"  P50 (Median): {stats['p50']}")
            print(f"  P75: {stats['p75']}")
            print(f"  P90: {stats['p90']}")
            print(f"  P95: {stats['p95']}")
            print(f"  P99: {stats['p99']}")
            print(f"  Max: {stats['max']}")
            print(f"\n  ğŸ“Œ Recommended max_length:")
            print(f"    Conservative (90%): {stats['recommended_max_lengths']['conservative']}")
            print(f"    Balanced (95%): {stats['recommended_max_lengths']['balanced']}")
            print(f"    Aggressive (99%): {stats['recommended_max_lengths']['aggressive']}")
            
        except Exception as e:
            print(f"  âŒ Error loading {model_name}: {e}")
            results[model_key] = {'error': str(e)}
    
    # ê²°ê³¼ ì €ì¥
    output_file = output_dir / 'token_analysis_results.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… Results saved to: {output_file}")
    
    # ë¶„í¬ ì‹œê°í™”
    if lengths_dict:
        plot_path = output_dir / 'token_distribution.png'
        plot_distribution(lengths_dict, str(plot_path))
    
    # ê¶Œì¥ ì„¤ì • ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“‹ ê¶Œì¥ max_length ì„¤ì • (Sweepìš©)")
    print("=" * 60)
    
    if 'klue/roberta-base' in results and 'error' not in results['klue/roberta-base']:
        base_stats = results['klue/roberta-base']
        print(f"""
sweep_config.yamlì— ì‚¬ìš©í•  ê°’:
  max_length:
    values: [{base_stats['p90']}, {base_stats['p95']}, {base_stats['p99']}]
""")
    
    return results


if __name__ == '__main__':
    results = main()
